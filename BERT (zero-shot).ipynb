{"cells":[{"cell_type":"markdown","source":["# BERT (zero-shot) for MLM"],"metadata":{"id":"udTPGQMDHnIZ"},"id":"udTPGQMDHnIZ"},{"cell_type":"code","execution_count":null,"id":"ffc2d7e4-64d9-4bb8-bad4-d38719c1ae1d","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"ffc2d7e4-64d9-4bb8-bad4-d38719c1ae1d"},"outputs":[],"source":["!pip install torch"]},{"cell_type":"code","execution_count":null,"id":"d74fa554-6a85-4496-b13d-3792cb6cce88","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"d74fa554-6a85-4496-b13d-3792cb6cce88"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"id":"31c733cd-af1c-4d3a-a76d-8e93307d6961","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"31c733cd-af1c-4d3a-a76d-8e93307d6961"},"outputs":[],"source":["!pip install matplotlib"]},{"cell_type":"markdown","id":"2d351443-f3c3-4af0-8030-949c1afec5c5","metadata":{"tags":[],"id":"2d351443-f3c3-4af0-8030-949c1afec5c5"},"source":["## Install required libraries"]},{"cell_type":"code","execution_count":null,"id":"dadf4619-56d7-4ac0-b817-48e43137cee9","metadata":{"id":"dadf4619-56d7-4ac0-b817-48e43137cee9"},"outputs":[],"source":["import sys\n","sys.path.insert(0,'/export/home/wei-ling.liao/.local/lib/python3.10/site-packages')"]},{"cell_type":"code","execution_count":null,"id":"3b66aed1-08d1-4ec5-a200-875cb7d503ac","metadata":{"id":"3b66aed1-08d1-4ec5-a200-875cb7d503ac"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re"]},{"cell_type":"code","execution_count":null,"id":"fa2d445f-249e-4103-b0a7-708ac3e5380b","metadata":{"id":"fa2d445f-249e-4103-b0a7-708ac3e5380b"},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","from transformers.models.bert.modeling_bert import BertModel\n","#from transformers import BertTokenizer, BertModel\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Load pre-trained model\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model.eval()\n","# Load pre-trained model tokenizer (vocabulary) ##IMPORTANT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","id":"fe8cb596-20bc-4fe9-910c-f52cd2100b61","metadata":{"tags":[],"id":"fe8cb596-20bc-4fe9-910c-f52cd2100b61"},"source":["## Load data from csv file"]},{"cell_type":"code","execution_count":null,"id":"2259c441-7035-4ddf-ae5b-4ed3085f1547","metadata":{"id":"2259c441-7035-4ddf-ae5b-4ed3085f1547"},"outputs":[],"source":["# Load the CSV file\n","csv_path = 'HumanDesignQue.csv'\n","df = pd.read_csv(csv_path)\n","\n","# Display the first few rows of the DataFrame\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"86aeb19e-5b5e-4c14-83c3-b46ba90e31b7","metadata":{"id":"86aeb19e-5b5e-4c14-83c3-b46ba90e31b7","outputId":"957ca5bd-6b65-4a85-be26-15abfd77846c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>metaphors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Thomas is a _ lark.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The library is a _ grave.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Are you feeling ill? You are a _ ghost.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>She was a _ mouse.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The cave was a _ night so we could not see any...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           metaphors\n","0                                Thomas is a _ lark.\n","1                          The library is a _ grave.\n","2            Are you feeling ill? You are a _ ghost.\n","3                                 She was a _ mouse.\n","4  The cave was a _ night so we could not see any..."]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["df[['metaphors']].head()"]},{"cell_type":"markdown","id":"ee2bd8c5-f7d8-4fde-b4ce-91aac6080ff2","metadata":{"id":"ee2bd8c5-f7d8-4fde-b4ce-91aac6080ff2"},"source":["## Input preparation"]},{"cell_type":"code","execution_count":null,"id":"16365dca-b6c5-415b-9b14-634ec8dc1c4d","metadata":{"id":"16365dca-b6c5-415b-9b14-634ec8dc1c4d"},"outputs":[],"source":["def prep_input(input_sents, tokenizer,bert=True):\n","    for sent in input_sents:\n","        text = []\n","        masked_tok = '[MASK]'\n","\n","        # replace masked token '_' with [MASK]\n","        sent = re.sub('_', masked_tok, sent)\n","\n","        # Split sentences and process each one\n","        #sentences = sent.strip().split('.')\n","        sentences = re.split(r'(?<=[.!?])\\s+', sent.strip())\n","\n","        for i, sentence in enumerate(sentences):\n","            # Add [CLS] before the first sentence\n","            if i == 0:\n","                text.append('[CLS]')\n","\n","            # Tokenize the sentence and add to the list\n","            text += sentence.strip().split()\n","\n","            # Add [SEP] after each sentence (except the last one)\n","            if i < len(sentences) - 1:\n","                text.append('[SEP]')\n","\n","        if sentences[-1].endswith('.'): # Add [SEP] to the last sentence of an input\n","            text.append('[SEP]')\n","\n","        text = ' '.join(text)\n","        tokenized_text = tokenizer.tokenize(text)\n","        #print(tokenized_text)\n","\n","        # Find the index of the masked token\n","        masked_index = tokenized_text.index(masked_tok) if masked_tok in tokenized_text else None\n","\n","        # Convert tokens to indices\n","        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","\n","        # Create a tensor for model input\n","        tokens_tensor = torch.tensor([indexed_tokens])\n","\n","        yield tokens_tensor, masked_index, tokenized_text\n"]},{"cell_type":"code","execution_count":null,"id":"1c7256dd-ff39-4dea-87fd-581e4d99a59a","metadata":{"tags":[],"id":"1c7256dd-ff39-4dea-87fd-581e4d99a59a"},"outputs":[],"source":["# Testing the prep_input function\n","input_sentences = [\n","    \"Thomas is a _ lark.\",\n","    \"The library is a _ grave.\",\n","    \"Are you feeling ill? You are a _ ghost.\"\n","    ]\n","\n","# Call the prep_input function for each sentence\n","for tokens_tensor, masked_index, tokenized_text in prep_input(input_sentences, tokenizer):\n","    print(\"Tokens Tensor:\", tokens_tensor)\n","    print(\"Masked Index:\", masked_index)\n","    print(\"Tokenized Text:\", tokenized_text)\n","    print(\"=\" * 50)"]},{"cell_type":"markdown","id":"2683fe5b-5eef-437a-a7dc-8fb88aea33c3","metadata":{"id":"2683fe5b-5eef-437a-a7dc-8fb88aea33c3"},"source":["## Get predictions of words"]},{"cell_type":"code","execution_count":null,"id":"f5125698-85dc-4a6b-b4a8-64ab70b8ae5b","metadata":{"tags":[],"id":"f5125698-85dc-4a6b-b4a8-64ab70b8ae5b"},"outputs":[],"source":["def get_predictions(input_sents, model, Tokenizer, k=5, bert=True):\n","    token_preds = [] # List to store the top-k predicted tokens for each input sentence\n","    token_probs = [] # List to store the associated probabilities\n","\n","    # Iterate over each input sentence and prepare it for the model\n","    for tokensTensor, maskedIndex, tokenizedText in prep_input(input_sents, tokenizer, bert=True):\n","        with torch.no_grad():\n","            predictions = model(tokensTensor)  # Get model predictions for the input sentence\n","\n","        predicted_tokens = []          # List to store the top-k predicted tokens\n","        predicted_token_probs = []     # List to store the associated probabilities\n","\n","        softmax_pred = torch.softmax(predictions[0][0,maskedIndex],0)  # Softmax probabilities for BERT\n","\n","        '''if maskedIndex >= softmax_pred.size(0):\n","            print(f\"Warning: masked index {maskedIndex} is out of bounds for the tensor with size {softmax_pred.size(0)}\")'''\n","\n","        # Check if maskedIndex is None\n","        if maskedIndex is None:\n","            print(\"Warning: maskedIndex is None.\")\n","            print(tokenizedText) # debug if maskedIndex cannot be found\n","            continue\n","\n","        # Check if maskedIndex is out of bounds\n","        if maskedIndex >= softmax_pred.size(0):\n","            print(f\"Warning: maskedIndex {mi} is out of bounds for the tensor with size {softpred.size(0)}\")\n","            continue\n","\n","        top_inds = torch.argsort(softmax_pred, descending=True)[:k].numpy()  # Indices of top-k predicted tokens\n","        top_probs = [softmax_pred[target_ind].item() for target_ind in top_inds]  # Probabilities of top-k predicted tokens\n","        top_tok_preds = tokenizer.convert_ids_to_tokens(top_inds)  # Convert indices to tokens\n","\n","        token_preds.append(top_tok_preds)\n","        token_probs.append(top_probs)\n","\n","\n","    return token_preds, token_probs\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0305e835-adc0-4578-b248-902a16e0b75a","metadata":{"tags":[],"id":"0305e835-adc0-4578-b248-902a16e0b75a"},"outputs":[],"source":["# Testing the get_predictions function\n","input_sentences = [\n","    \"Thomas is a _ lark.\",\n","    \"The library is so quiet. It is a _ grave.\",\n","    \"Are you feeling ill? You are a _ ghost.\",\n","    \"She was a _ mouse.\",\n","    \"The cave was a _ night so we could not anything.\"\n","]\n","\n","# Call the get_predictions function\n","token_preds, token_probs = get_predictions(input_sentences, model, tokenizer, k=5, bert=True)\n","\n","# Display the results\n","for i, (tokens, probs) in enumerate(zip(token_preds, token_probs)):\n","    print(f\"Input Sentence: {input_sentences[i]}\")\n","    print(f\"Top-k Predicted Tokens: {tokens}\")\n","    print(f\"Associated Probabilities: {probs}\")\n","    print(\"=\"*50)"]},{"cell_type":"markdown","id":"5c64cdda-076e-4e4e-839e-2d1e213c0c1e","metadata":{"id":"5c64cdda-076e-4e4e-839e-2d1e213c0c1e"},"source":["## Get prediction probabilities"]},{"cell_type":"code","execution_count":null,"id":"6436dd10-e644-4bc8-8fb0-6d5ea270e46c","metadata":{"id":"6436dd10-e644-4bc8-8fb0-6d5ea270e46c"},"outputs":[],"source":["def get_probabilities(input_sents,tgtlist,model,tokenizer,bert=True):\n","    token_probs = []\n","    for i,(tokensTensor, maskedIndex, tokenizedText) in enumerate(prep_input(input_sents,tokenizer,bert=bert)):\n","\n","        with torch.no_grad():\n","            predictions = model(tokensTensor)\n","\n","        tgt = tgtlist[i]\n","        softmax_pred = torch.softmax(predictions[0][0,maskedIndex],0)\n","\n","        try:\n","            tgt_ind = tokenizer.convert_tokens_to_ids([tgt])[0]\n","        except:\n","            this_tgt_prob = np.nan  ## If a target token is not found in vocabulary, the probability for that token is set to NumPy NaN\n","        else:\n","            this_tgt_prob = softmax_pred[tgt_ind].item()\n","\n","        token_probs.append(this_tgt_prob)\n","    return token_probs"]},{"cell_type":"code","execution_count":null,"id":"65d8e835-42b2-449e-a9e4-9b8d95f7ac72","metadata":{"id":"65d8e835-42b2-449e-a9e4-9b8d95f7ac72"},"outputs":[],"source":["## Testing the get_probabilities function\n","input_sentences = [\n","    \"Thomas is a _ lark.\",\n","    \"The library is a _ grave.\",\n","    \"Are you feeling ill? You are a _ ghost.\"\n","]\n","\n","# Sample target tokens\n","target_tokens = [\"happy\", \"silent\", \"pale\"]\n","\n","# Call the get_probabilities function\n","probs = get_probabilities(input_sentences, target_tokens, model, tokenizer, bert=True)\n","\n","# Display the results\n","for i, (sent, tgt, prob) in enumerate(zip(input_sentences, target_tokens, probs)):\n","    print(f\"Input Sentence: {sent}\")\n","    print(f\"Target Token: {tgt}\")\n","    print(f\"Probability: {prob}\")\n","    print(\"=\" * 50)"]},{"cell_type":"markdown","id":"5b293510-bd48-455c-ab7f-4e3199d32914","metadata":{"id":"5b293510-bd48-455c-ab7f-4e3199d32914"},"source":["## Get model responses"]},{"cell_type":"code","execution_count":null,"id":"486acb08-90ff-4b9b-8972-3d0089406aaf","metadata":{"id":"486acb08-90ff-4b9b-8972-3d0089406aaf"},"outputs":[],"source":["def get_model_responses(inputlist,tgtlist,modeliname,model,tokenizer,k=5,bert=True):\n","    top_preds,top_probs = tp.get_predictions(inputlist,model,tokenizer,k=k,bert=bert)\n","    tgt_probs = tp.get_probabilities(inputlist,tgtlist,model,tokenizer,bert=bert)\n","\n","    return top_preds,top_probs,tgt_probs"]},{"cell_type":"markdown","source":["# Implementation"],"metadata":{"id":"UPATCCltErlI"},"id":"UPATCCltErlI"},{"cell_type":"markdown","id":"8d9173db-8833-4f60-89ed-3ac6b99918cf","metadata":{"id":"8d9173db-8833-4f60-89ed-3ac6b99918cf"},"source":["### 1. Functions Implmentation on HumanQue dataset"]},{"cell_type":"code","execution_count":null,"id":"47af812d-c548-4ad7-88c3-5b0d1f97035b","metadata":{"tags":[],"id":"47af812d-c548-4ad7-88c3-5b0d1f97035b"},"outputs":[],"source":["# Read input sentences from a CSV file (adjust the file path and column names)\n","csv_file_path = 'HumanDesignQue.csv'\n","df = pd.read_csv(csv_file_path)\n","\n","# Extract the 'Sentence' column as input sentences\n","input_sentences = df['metaphors'].tolist()\n"]},{"cell_type":"code","execution_count":null,"id":"02cddbbf-6812-4534-a4e0-caf5824ed192","metadata":{"tags":[],"id":"02cddbbf-6812-4534-a4e0-caf5824ed192"},"outputs":[],"source":["# Call the get_predictions function\n","token_preds, token_probs = get_predictions(input_sentences, model, tokenizer, k=5, bert=True)\n","\n","# Create a DataFrame to store the results\n","results_df = pd.DataFrame({\n","    'Input Sentence': input_sentences,\n","    'Top-k Predicted Tokens': token_preds,\n","    'Associated Probabilities': token_probs\n","})\n","\n"]},{"cell_type":"code","execution_count":null,"id":"97da1085-a436-4c4e-95b3-589b757ccce0","metadata":{"tags":[],"id":"97da1085-a436-4c4e-95b3-589b757ccce0"},"outputs":[],"source":["# Save the DataFrame to a new CSV file (adjust the file path)\n","output_csv_file_path = 'HumanQue_results.csv'\n","results_df.to_csv(output_csv_file_path, index=False)\n","\n","print(f\"Results saved to {output_csv_file_path}\")"]},{"cell_type":"markdown","id":"81e5bb2e-b110-45c3-85d7-f0d53a17444b","metadata":{"id":"81e5bb2e-b110-45c3-85d7-f0d53a17444b"},"source":["### 2. Functions Implementation on General Corpus dataset"]},{"cell_type":"code","execution_count":null,"id":"32c2956e-2c21-491a-83c7-e219f7703794","metadata":{"id":"32c2956e-2c21-491a-83c7-e219f7703794"},"outputs":[],"source":["# Read input sentences from a CSV file (adjust the file path and column names)\n","csv_file_path = 'GeneralCorpus.csv'\n","df = pd.read_csv(csv_file_path)\n","\n","# Extract the 'Sentence' column as input sentences\n","input_sentences = df['metaphor'].tolist()"]},{"cell_type":"code","execution_count":null,"id":"2d1608be-d7fb-4ab4-950b-fc3cddd08b34","metadata":{"id":"2d1608be-d7fb-4ab4-950b-fc3cddd08b34"},"outputs":[],"source":["# Call the get_predictions function\n","token_preds, token_probs = get_predictions(input_sentences, model, tokenizer, k=5, bert=True)\n","\n","# Create a DataFrame to store the results\n","results_df = pd.DataFrame({\n","    'Input Sentence': input_sentences,\n","    'Top-k Predicted Tokens': token_preds,\n","    'Associated Probabilities': token_probs\n","})"]},{"cell_type":"code","execution_count":null,"id":"cab4a2e2-6f03-4aa6-afd4-bb38d6fef787","metadata":{"id":"cab4a2e2-6f03-4aa6-afd4-bb38d6fef787"},"outputs":[],"source":["# Save the DataFrame to a new CSV file (adjust the file path)\n","output_csv_file_path = 'GeneralCorpus_results.csv'\n","results_df.to_csv(output_csv_file_path, index=False)\n","\n","print(f\"Results saved to {output_csv_file_path}\")"]},{"cell_type":"markdown","id":"c513be8c-0911-42b0-8a43-e63f788ad82e","metadata":{"id":"c513be8c-0911-42b0-8a43-e63f788ad82e"},"source":["### 3. Functions Implementation on non-metaphor dataset"]},{"cell_type":"code","execution_count":null,"id":"6dac22f5-1ebb-4b5a-addc-98103989f5e6","metadata":{"id":"6dac22f5-1ebb-4b5a-addc-98103989f5e6"},"outputs":[],"source":["# Read input sentences from a CSV file (adjust the file path and column names)\n","csv_file_path = 'non-metaphor_COCA.csv'\n","df = pd.read_csv(csv_file_path)\n","\n","# Extract the 'Sentence' column as input sentences\n","input_sentences = df['Non-Metaphor'].tolist()"]},{"cell_type":"code","execution_count":null,"id":"40161dd9-021e-4958-a3a3-be574a5898b9","metadata":{"id":"40161dd9-021e-4958-a3a3-be574a5898b9"},"outputs":[],"source":["# Call the get_predictions function\n","token_preds, token_probs = get_predictions(input_sentences, model, tokenizer, k=5, bert=True)\n","\n","# Create a DataFrame to store the results\n","results_df = pd.DataFrame({\n","    'Input Sentence': input_sentences,\n","    'Top-k Predicted Tokens': token_preds,\n","    'Associated Probabilities': token_probs\n","})"]},{"cell_type":"code","execution_count":null,"id":"be8acd5c-3ab4-484c-84e6-5fc113692a8e","metadata":{"id":"be8acd5c-3ab4-484c-84e6-5fc113692a8e"},"outputs":[],"source":["# Save the DataFrame to a new CSV file (adjust the file path)\n","output_csv_file_path = 'non-metaphor_results.csv'\n","results_df.to_csv(output_csv_file_path, index=False)\n","\n","print(f\"Results saved to {output_csv_file_path}\")"]},{"cell_type":"markdown","source":["# Evaluation: Attention Mechanism Visualization (zero-shot)"],"metadata":{"id":"EF85X8JxE_3C"},"id":"EF85X8JxE_3C"},{"cell_type":"markdown","source":["### 1. Human-Designed Questions Metaphor"],"metadata":{"id":"yxNZk1pSGvcZ"},"id":"yxNZk1pSGvcZ"},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Load the tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased', output_attentions=True)\n","\n","# Prepare the input\n","text = \"Are you feeling ill? You are a [MASK] ghost.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()\n"],"metadata":{"id":"q5MgZHnUG2NP"},"id":"q5MgZHnUG2NP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the input\n","text = \"Peter is a [MASK] beanpole.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()\n"],"metadata":{"id":"ZKMmH0ooG7M2"},"id":"ZKMmH0ooG7M2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the input\n","text = \"Jason was a [MASK] peacock after winning first place in the swimming competition.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()"],"metadata":{"id":"p_3DhvfVHAtW"},"id":"p_3DhvfVHAtW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. General Corpus Metaphor"],"metadata":{"id":"B69vxdAkHJRM"},"id":"B69vxdAkHJRM"},{"cell_type":"code","source":["# Prepare the input\n","text = \"As long as you can drive away from their shop without toppling over or crashing into something or someone, you’re a [MASK] bird.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()"],"metadata":{"id":"HwnslMVyHLuG"},"id":"HwnslMVyHLuG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the input\n","text = \"As I got closer to finally see what was in store for me, Dan said my eyes were the [MASK] saucers and my jaw dropped.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()"],"metadata":{"id":"D0k4aAzuHQza"},"id":"D0k4aAzuHQza","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Non-metaphor"],"metadata":{"id":"yxaGfpv2HWL5"},"id":"yxaGfpv2HWL5"},{"cell_type":"code","source":["# Prepare the input\n","text = \"Now it was Cara’s turn to give back. She drew a [MASK] breath and opened her hands, which had been clenched into fists.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()"],"metadata":{"id":"ipUkXdtaHYow"},"id":"ipUkXdtaHYow","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the input\n","text = \"I was unbearably hot. I flung the blanket off and sat up. My [MASK] feet found relief on the cold hardwood floor, and I rubbed my eyes.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()"],"metadata":{"id":"IYA8zkmtHb1E"},"id":"IYA8zkmtHb1E","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the input\n","text = \"Brad felt like he was watching a train wreck develop in [MASK] motion and was powerless to stop it.\"\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","\n","# Get predictions and attention weights\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    attention = outputs['attentions']\n","    prediction_scores = outputs['logits']\n","    predicted_id = torch.argmax(prediction_scores[0, mask_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n","\n","# Aggregate the attention weights across all layers and heads\n","all_layers_attention = torch.stack(attention).mean(dim=0)  # Average over layers\n","all_heads_attention = all_layers_attention.mean(dim=1)  # Average over heads\n","avg_attention = all_heads_attention[0].detach().numpy()  # For the first (and only) input in the batch\n","\n","# Plot the aggregated attention weights as a heatmap\n","plt.figure(figsize=(5,4))\n","sns.heatmap(avg_attention, annot=False, cmap='viridis', xticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()), yticklabels=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n","plt.title(f'Aggregated Attention, Predicted: {predicted_token}')\n","plt.show()"],"metadata":{"id":"g7siYljrHfQH"},"id":"g7siYljrHfQH","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}